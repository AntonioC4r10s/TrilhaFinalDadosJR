![C√≥digo Certo Coders](https://i.imgur.com/MAyFXGV.jpg)

# üìö Trilha Final Ci√™ncia de Dados Jr

O objetivo deste projeto √© realizar uma an√°lise detalhada dos dados coletados em nossa pesquisa de satisfa√ß√£o da comunidade nos meses de junho e julho de 2024. Utilizaremos ci√™ncia de dados para extrair insights significativos que possam orientar tanto estrat√©gias imediatas quanto decis√µes a longo prazo.

Conforme a nossa comunidade cresce em n√∫meros com velocidade, torna-se crucial entendermos e atendermos √†s expectativas e necessidades dos nossos membros. Para isso, realizamos periodicamente pesquisas de satisfa√ß√£o, visando obter feedback valioso que nos guie na melhoria cont√≠nua dos nossos workshops, mentorias e experi√™ncia geral oferecida. Vale lembrar que os participantes desta trilha devem escolher apenas uma √°rea de interesse espec√≠fica. 

Este projeto realiza a extra√ß√£o, transforma√ß√£o e carga (ETL) dos dados de satisfa√ß√£o dos membros de uma comunidade de c√≥digo. 

## Tecnologias Utilizadas

- **gspread**: Para interagir com Google Sheets.
- **oauth2client**: Para autentica√ß√£o OAuth 2.0.
- **pandas**: Para manipula√ß√£o e an√°lise de dados.
- **requests**: Para fazer requisi√ß√µes HTTP.
- **psycopg2-binary**: Para conectar ao banco de dados PostgreSQL.
- **Apache Airflow**: Para orquestrar o fluxo de trabalho de ETL.

## Trilha Escolhida: Engenheiro de Dados
### Engenheiro(a) de Dados I - N√≠vel J√∫nior
#### Desafios e Expectativas:
- Realizar tarefas b√°sicas de ETL e manuten√ß√£o de bancos de dados.
- Criar e otimizar consultas SQL simples.
- Trabalhar com ferramentas e pr√°ticas de modelagem de dados b√°sicas.
- Implementar processos de ETL para integrar dados de diferentes fontes.
- Demonstrar conhecimento b√°sico em modelagem dimensional e estrutura de dados.
- Manter e atualizar o banco de dados com efici√™ncia.
### Engenheiro(a) de Dados II - N√≠vel Intermedi√°rio
#### Desafios e Expectativas:
- Desenvolver processos de ETL mais complexos e otimizar modelos de dados.
- Trabalhar com ferramentas de integra√ß√£o de dados e plataformas na nuvem.
- Realizar modelagem de dados para suportar an√°lises avan√ßadas.
- Criar e otimizar pipelines de ETL que integrem dados de v√°rias fontes.
- Implementar solu√ß√µes de modelagem de dados que suportem as necessidades anal√≠ticas.
- Utilizar plataformas na nuvem e ferramentas de BI para melhorar a performance e escalabilidade.
### Engenheiro(a) de Dados III - N√≠vel Avan√ßado
#### Desafios e Expectativas:
- Projetar e implementar arquiteturas de dados complexas e escal√°veis.
- Resolver problemas de performance em ambientes de dados na nuvem e h√≠bridos.
- Propor e implementar solu√ß√µes de governan√ßa de dados e pr√°ticas avan√ßadas de modelagem.
- Desenvolver arquiteturas de dados eficientes e escal√°veis que suportem grandes volumes de dados.
- Resolver problemas de performance e otimizar processos em ambientes de dados complexos.
- Propor e implementar estrat√©gias avan√ßadas de governan√ßa e integra√ß√£o de dados.
  


## Instala√ß√£o

1. Clone o reposit√≥rio:

```bash
git clone https://github.com/AntonioC4r10s/TrilhaFinalDadosJR.git
cd seu-projeto
```
2. Instale as depend√™ncias (caso queira realizar altera√ß√µes e melhorias):
```bash
pip install -r requirements.txt
```
3. Execute a constru√ß√£o e execu√ß√£o das images Docker:
```bash
docker-compose up --build
```
Para parar e iniciar respectivamente nas demais vezes, utilize apenas:
```bash
docker-compose down
```
```bash
docker-compose up -d
```

## Estrutura do Projeto
```css
‚îú‚îÄ‚îÄ dags
‚îÇ   ‚îî‚îÄ‚îÄ etl_dados_pesquisa.py
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îú‚îÄ‚îÄ extract.py
‚îÇ   ‚îú‚îÄ‚îÄ transform.py
‚îÇ   ‚îú‚îÄ‚îÄ load_to_gsheets.py
‚îÇ   ‚îú‚îÄ‚îÄ load_to_postgresql.py
‚îÇ   ‚îú‚îÄ‚îÄ data
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.db
‚îÇ   ‚îî‚îÄ‚îÄ credentials
‚îÇ       ‚îú‚îÄ‚îÄ gsheets_acess.py
‚îÇ       ‚îî‚îÄ‚îÄ pgdb_acess.py
‚îú‚îÄ‚îÄ logs
‚îú‚îÄ‚îÄ plugins
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îî‚îÄ‚îÄ README.md
```
## Executando o Projeto
Etapas de Execu√ß√£o:
- Extra√ß√£o: Coleta de dados de satisfa√ß√£o da comunidade.
- Transforma√ß√£o: Processamento e formata√ß√£o dos dados.
- Carga: Armazenamento dos dados no Google Sheets e PostgreSQL.

Etapas de Execu√ß√£o que formam aa DAG do projeto:
```python
def extract_task():
    extract()

def transform_task(**context):
    df = transform()
    context['task_instance'].xcom_push(key='transformed_data', value=df)

def load_task_gsheets(**context):
    df = context['task_instance'].xcom_pull(task_ids='transform', key='transformed_data')
    load_to_gsheets(df)

def load_task_postgresql(**context):
    df = context['task_instance'].xcom_pull(task_ids='transform', key='transformed_data')
    load_to_postgresql(df)
```

## Arquitetura do ETL

- Extra√ß√£o
  - Na etapa de extra√ß√£o, os arquivos CSV s√£o baixados diretamente do reposit√≥rio GitHub do administrador. Esses arquivos s√£o salvos em um banco de dados SQLite, que serve como nosso Data Lake. A escolha pelo SQLite foi motivada pela sua simplicidade, facilidade de uso e custo zero, sendo ideal para um projeto com infraestrutura limitada.

- Transforma√ß√£o
  - Na fase de transforma√ß√£o, v√°rias opera√ß√µes s√£o realizadas nos dados para garantir consist√™ncia e padroniza√ß√£o. Em particular, as strings s√£o corrigidas para um formato onde todas as palavras come√ßam com letra mai√∫scula, exceto por conectivos como "de", "da", "dos", "das", etc. Por exemplo, a string "academia de gin√°stica" seria transformada em "Academia de Gin√°stica".

- Carga
  - Ap√≥s a transforma√ß√£o, os dados s√£o carregados em dois destinos diferentes:

  - Google Sheets: Foi utilizado o Google Sheets como um Data Warehouse devido √† sua acessibilidade e facilidade de uso para visualiza√ß√£o e an√°lise de dados.
  - PostgreSQL: Os dados tamb√©m s√£o carregados em uma inst√¢ncia do PostgreSQL, criada no Tembo, para permitir consultas mais complexas e integra√ß√£o com outras ferramentas de an√°lise de dados.

Esta imagem ilustra a arquitetura do pipeline de dados, destacando cada uma das etapas mencionadas e a intera√ß√£o entre os diferentes componentes do sistema.

#### Imagem da Arquitetura do pipeline

![Imagem da DAG no Airflow](img/img1.png)

### Orquestra√ß√£o
Todo o processo de ETL √© gerenciado pelo Apache Airflow, que coordena as tarefas de extra√ß√£o, transforma√ß√£o e carga de maneira eficiente e escal√°vel. O uso do Airflow permite monitorar e agendar as tarefas, garantindo que o pipeline de dados funcione de maneira automatizada e sem interrup√ß√µes.

#### Imagem da DAG no Airflow

![Imagem da DAG no Airflow](img/img1.png)


## Acesso ao Data Warehouse
Voc√™ pode acessar o Data Warehouse atrav√©s das seguintes op√ß√µes:
- CSV do Google Sheets
    - Os dados coletados est√£o dispon√≠veis em formato CSV. Para acessar o arquivo, clique no link abaixo:
    https://bit.ly/CommunityFeedbackDW
- Voc√™ pode usar a biblioteca pandas para ler o CSV diretamente do Google Sheets. Aqui est√° um exemplo de como fazer isso:

```python
import pandas as pd

# URL do CSV do Google Sheets
csv_url = 'https://bit.ly/CommunityFeedbackDW'

# L√™ o CSV e carrega os dados em um DataFrame
df = pd.read_csv(csv_url)

# Exibe as primeiras linhas do DataFrame
print(df.head())
```
- Acesso ao PostgreSQL:
    - Para acessar o banco de dados PostgreSQL, utilize as seguintes credenciais:
      - Host: volubly-relieved-macaw.data-1.use1.tembo.io
      - Port: 5432
      - User: read_only_user
      - Password: 12345678
    - Certifique-se de que voc√™ tenha um cliente PostgreSQL instalado e use as credenciais acima para se conectar e explorar os dados.

## Respondendo as  perguntas propostas
1. Nossa infraestrutura de dados est√° impactando a performance da an√°lise de feedbacks. Como voc√™ redesenharia a arquitetura para melhorar a efici√™ncia?
Para melhorar a efici√™ncia da an√°lise de feedbacks, eu come√ßaria realizando uma avalia√ß√£o da infraestrutura atual para identificar gargalos. Um redesenho poderia incluir:
    - Implementa√ß√£o de um Data Warehouse: Centralizar os dados em um data warehouse que seja otimizado para consultas, utilizando um modelo de dados adequado (como star ou snowflake).
    - ETL otimizado: Melhorar o processo de ETL para garantir que os dados sejam processados de maneira eficiente, utilizando ferramentas de integra√ß√£o que suportem a carga incremental em vez de cargas totais.
    - Uso de armazenamento em nuvem: Considerar o uso de solu√ß√µes em nuvem escal√°veis.

2. Nosso processo de ETL est√° gerando dados duplicados e inconsistentes. Como voc√™ resolveria esses problemas para garantir a integridade dos dados?

- Para resolver problemas de duplica√ß√£o e inconsist√™ncia no ETL:
  - Implementar valida√ß√µes de dados: Durante a extra√ß√£o e transforma√ß√£o, aplicar regras de valida√ß√£o para detectar duplicatas, como verificar chaves prim√°rias ou hashes.

3. Nossa ferramenta de visualiza√ß√£o de dashboards est√° lenta e o nosso time detectou que o problema est√° na infraestrutura de dados. Como voc√™ abordaria esta situa√ß√£o do ponto vista de arquitetura de dados?
- Abordaria a situa√ß√£o da seguinte forma:
  - Analisar as consultas feitas pelos dashboards e otimiz√°-las, utilizando √≠ndices e evitando joins desnecess√°rios.
  - Considerar o uso de views materializadas para pr√©-computar resultados e reduzir o tempo de resposta nas consultas mais frequentes.
 
4. Descreva como voc√™ projetaria um pipeline de ETL para integrar os feedbacks coletados em diferentes formatos (planilhas, formul√°rios online, etc.) em um √∫nico data warehouse.
Eu projetaria um pipeline de ETL da seguinte maneira:
    - Extra√ß√£o: Utilizar conectores para diferentes fontes de dados (APIs, arquivos CSV, etc.).
    - Transforma√ß√£o: Normalizar os dados coletados, garantindo que todos os formatos sejam convertidos para um padr√£o comum (por exemplo, mesmo esquema de colunas).
    - Valida√ß√£o: Aplicar regras de valida√ß√£o para garantir que os dados sejam consistentes e completos antes de carreg√°-los.
    - Carga: Inserir os dados transformados em um data warehouse.
    - Orquestra√ß√£o: Automatizar e agendar as tasks e um espa√ßo de tempo que fa√ßa sentido para a aplica√ß√£o.

5. Nossa √°rea operacional necessita de informa√ß√µes em tempo real, por√©m os gestores da comunidade acompanham somente informa√ß√µes de KPIs mensais, alegam que isso √© desnecess√°rio e acarretaria custos. Qual √© o seu posicionamento sobre isso?
    - Acredito que, embora os KPIs mensais sejam valiosos, informa√ß√µes em tempo real podem fornecer insights cr√≠ticos que permitem decis√µes mais r√°pidas e informadas. Contudo √© necessario avaliar se realmente a escalada de custo faz sentido na vis√£o geral das analises.

6. Como voc√™ implementaria pr√°ticas de governan√ßa de dados para assegurar que os feedbacks sejam armazenados e utilizados de maneira segura e conforme as pol√≠ticas da comunidade?
    - Pol√≠ticas de acesso: Definir quem tem acesso aos dados e em que n√≠vel, garantindo que apenas pessoas autorizadas possam visualizar ou modificar informa√ß√µes sens√≠veis (a maior partes dos banco de dados mais utilizados j√° possuem tal propriedade).

7. Nosso time que est√° focado em Governan√ßa de Dados alega que documentar os processos √© mais importante do que refatorar os mais de 500 scripts que est√£o funcionando com lentid√£o. Como voc√™ atuaria neste impasse, se tivesse que priorizar o trabalho?
- Neste impasse, eu buscaria um equil√≠brio entre documenta√ß√£o e melhoria de desempenho:
    - Documenta√ß√£o m√≠nima vi√°vel: Criar documenta√ß√£o b√°sica para os scripts enquanto trabalha na refatora√ß√£o.

--- 

## Contribui√ß√µes e Contatos
- **LinkedIn**: [Antonio Junior](https://www.linkedin.com/in/antoniojuniortec/)
- **GitHub**: [AntonioC4r10s](https://github.com/AntonioC4r10s)

Sinta-se √† vontade para abrir issues ou pull requests no reposit√≥rio. Estou ansioso para colaborar com voc√™!
