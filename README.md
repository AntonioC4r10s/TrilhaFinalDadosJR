![CÃ³digo Certo Coders](https://i.imgur.com/MAyFXGV.jpg)

# ğŸ“š Trilha Final CiÃªncia de Dados Jr

O objetivo deste projeto Ã© realizar uma anÃ¡lise detalhada dos dados coletados em nossa pesquisa de satisfaÃ§Ã£o da comunidade nos meses de junho e julho de 2024. Utilizaremos ciÃªncia de dados para extrair insights significativos que possam orientar tanto estratÃ©gias imediatas quanto decisÃµes a longo prazo.

Conforme a nossa comunidade cresce em nÃºmeros com velocidade, torna-se crucial entendermos e atendermos Ã s expectativas e necessidades dos nossos membros. Para isso, realizamos periodicamente pesquisas de satisfaÃ§Ã£o, visando obter feedback valioso que nos guie na melhoria contÃ­nua dos nossos workshops, mentorias e experiÃªncia geral oferecida. Vale lembrar que os participantes desta trilha devem escolher apenas uma Ã¡rea de interesse especÃ­fica. 

Este projeto realiza a extraÃ§Ã£o, transformaÃ§Ã£o e carga (ETL) dos dados de satisfaÃ§Ã£o dos membros de uma comunidade de cÃ³digo. 

## Tecnologias Utilizadas

- **gspread**: Para interagir com Google Sheets.
- **oauth2client**: Para autenticaÃ§Ã£o OAuth 2.0.
- **pandas**: Para manipulaÃ§Ã£o e anÃ¡lise de dados.
- **requests**: Para fazer requisiÃ§Ãµes HTTP.
- **psycopg2-binary**: Para conectar ao banco de dados PostgreSQL.
- **Apache Airflow**: Para orquestrar o fluxo de trabalho de ETL.

## Trilha Escolhida: Engenheiro de Dados
### Engenheiro(a) de Dados I - NÃ­vel JÃºnior
#### Desafios e Expectativas:
- Realizar tarefas bÃ¡sicas de ETL e manutenÃ§Ã£o de bancos de dados.
- Criar e otimizar consultas SQL simples.
- Trabalhar com ferramentas e prÃ¡ticas de modelagem de dados bÃ¡sicas.
- Implementar processos de ETL para integrar dados de diferentes fontes.
- Demonstrar conhecimento bÃ¡sico em modelagem dimensional e estrutura de dados.
- Manter e atualizar o banco de dados com eficiÃªncia.
### Engenheiro(a) de Dados II - NÃ­vel IntermediÃ¡rio
#### Desafios e Expectativas:
- Desenvolver processos de ETL mais complexos e otimizar modelos de dados.
- Trabalhar com ferramentas de integraÃ§Ã£o de dados e plataformas na nuvem.
- Realizar modelagem de dados para suportar anÃ¡lises avanÃ§adas.
- Criar e otimizar pipelines de ETL que integrem dados de vÃ¡rias fontes.
- Implementar soluÃ§Ãµes de modelagem de dados que suportem as necessidades analÃ­ticas.
- Utilizar plataformas na nuvem e ferramentas de BI para melhorar a performance e escalabilidade.
### Engenheiro(a) de Dados III - NÃ­vel AvanÃ§ado
#### Desafios e Expectativas:
- Projetar e implementar arquiteturas de dados complexas e escalÃ¡veis.
- Resolver problemas de performance em ambientes de dados na nuvem e hÃ­bridos.
- Propor e implementar soluÃ§Ãµes de governanÃ§a de dados e prÃ¡ticas avanÃ§adas de modelagem.
- Desenvolver arquiteturas de dados eficientes e escalÃ¡veis que suportem grandes volumes de dados.
- Resolver problemas de performance e otimizar processos em ambientes de dados complexos.
- Propor e implementar estratÃ©gias avanÃ§adas de governanÃ§a e integraÃ§Ã£o de dados.
  


## InstalaÃ§Ã£o

1. Clone o repositÃ³rio:

```bash
git clone https://github.com/AntonioC4r10s/TrilhaFinalDadosJR.git
cd seu-projeto
```
2. Instale as dependÃªncias (caso queira realizar alteraÃ§Ãµes e melhorias):
```bash
pip install -r requirements.txt
```
3. Execute a construÃ§Ã£o e execuÃ§Ã£o das images Docker:
```bash
docker-compose up --build
```
Para parar e iniciar respectivamente nas demais vezes, utilize apenas:
```bash
docker-compose down
```
```bash
docker-compose up -d
```

## Estrutura do Projeto
```css
â”œâ”€â”€ dags
â”‚   â””â”€â”€ etl_dados_pesquisa.py
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ load_to_gsheets.py
â”‚   â”œâ”€â”€ load_to_postgresql.py
â”‚   â”œâ”€â”€ data
â”‚   â”‚   â””â”€â”€ data.db
â”‚   â””â”€â”€ credentials
â”‚       â”œâ”€â”€ gsheets_acess.py
â”‚       â””â”€â”€ pgdb_acess.py
â”œâ”€â”€ logs
â”œâ”€â”€ plugins
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```
## Executando o Projeto
Etapas de ExecuÃ§Ã£o:
- ExtraÃ§Ã£o: Coleta de dados de satisfaÃ§Ã£o da comunidade.
- TransformaÃ§Ã£o: Processamento e formataÃ§Ã£o dos dados.
- Carga: Armazenamento dos dados no Google Sheets e PostgreSQL.

Etapas de ExecuÃ§Ã£o que formam aa DAG do projeto:
```python
def extract_task():
    extract()

def transform_task(**context):
    df = transform()
    context['task_instance'].xcom_push(key='transformed_data', value=df)

def load_task_gsheets(**context):
    df = context['task_instance'].xcom_pull(task_ids='transform', key='transformed_data')
    load_to_gsheets(df)

def load_task_postgresql(**context):
    df = context['task_instance'].xcom_pull(task_ids='transform', key='transformed_data')
    load_to_postgresql(df)
```

## Arquitetura do ETL

- ExtraÃ§Ã£o
  - Na etapa de extraÃ§Ã£o, os arquivos CSV sÃ£o baixados diretamente do repositÃ³rio GitHub do administrador. Esses arquivos sÃ£o salvos em um banco de dados SQLite, que serve como nosso Data Lake. A escolha pelo SQLite foi motivada pela sua simplicidade, facilidade de uso e custo zero, sendo ideal para um projeto com infraestrutura limitada.

- TransformaÃ§Ã£o
  - Na fase de transformaÃ§Ã£o, vÃ¡rias operaÃ§Ãµes sÃ£o realizadas nos dados para garantir consistÃªncia e padronizaÃ§Ã£o. Em particular, as strings sÃ£o corrigidas para um formato onde todas as palavras comeÃ§am com letra maiÃºscula, exceto por conectivos como "de", "da", "dos", "das", etc. Por exemplo, a string "academia de ginÃ¡stica" seria transformada em "Academia de GinÃ¡stica".

- Carga
  - ApÃ³s a transformaÃ§Ã£o, os dados sÃ£o carregados em dois destinos diferentes:

  - Google Sheets: Foi utilizado o Google Sheets como um Data Warehouse devido Ã  sua acessibilidade e facilidade de uso para visualizaÃ§Ã£o e anÃ¡lise de dados.
  - PostgreSQL: Os dados tambÃ©m sÃ£o carregados em uma instÃ¢ncia do PostgreSQL, criada no Tembo, para permitir consultas mais complexas e integraÃ§Ã£o com outras ferramentas de anÃ¡lise de dados.

Esta imagem ilustra a arquitetura do pipeline de dados, destacando cada uma das etapas mencionadas e a interaÃ§Ã£o entre os diferentes componentes do sistema.

#### Imagem da Arquitetura do pipeline

![Imagem da DAG no Airflow](img/img1.png)

### OrquestraÃ§Ã£o
Todo o processo de ETL Ã© gerenciado pelo Apache Airflow, que coordena as tarefas de extraÃ§Ã£o, transformaÃ§Ã£o e carga de maneira eficiente e escalÃ¡vel. O uso do Airflow permite monitorar e agendar as tarefas, garantindo que o pipeline de dados funcione de maneira automatizada e sem interrupÃ§Ãµes.

#### Imagem da DAG no Airflow

![Imagem da DAG no Airflow](img/img1.png)


## Acesso ao Data Warehouse
VocÃª pode acessar o Data Warehouse atravÃ©s das seguintes opÃ§Ãµes:
- CSV do Google Sheets
    - Os dados coletados estÃ£o disponÃ­veis em formato CSV. Para acessar o arquivo, clique no link abaixo:
    https://bit.ly/CommunityFeedbackDW
- VocÃª pode usar a biblioteca pandas para ler o CSV diretamente do Google Sheets. Aqui estÃ¡ um exemplo de como fazer isso:

```python
import pandas as pd

# URL do CSV do Google Sheets
csv_url = 'https://bit.ly/CommunityFeedbackDW'

# LÃª o CSV e carrega os dados em um DataFrame
df = pd.read_csv(csv_url)

# Exibe as primeiras linhas do DataFrame
print(df.head())
```
- Acesso ao PostgreSQL:
    - Para acessar o banco de dados PostgreSQL, utilize as seguintes credenciais:
      - Host: volubly-relieved-macaw.data-1.use1.tembo.io
      - Port: 5432
      - User: read_only_user
      - Password: 12345678
    - Certifique-se de que vocÃª tenha um cliente PostgreSQL instalado e use as credenciais acima para se conectar e explorar os dados.

## Respondendo as  perguntas propostas
1. Nossa infraestrutura de dados estÃ¡ impactando a performance da anÃ¡lise de feedbacks. Como vocÃª redesenharia a arquitetura para melhorar a eficiÃªncia?
Para melhorar a eficiÃªncia da anÃ¡lise de feedbacks, eu comeÃ§aria realizando uma avaliaÃ§Ã£o da infraestrutura atual para identificar gargalos. Um redesenho poderia incluir:
    - ImplementaÃ§Ã£o de um Data Warehouse: Centralizar os dados em um data warehouse que seja otimizado para consultas, utilizando um modelo de dados adequado (como star ou snowflake).
    - ETL otimizado: Melhorar o processo de ETL para garantir que os dados sejam processados de maneira eficiente, utilizando ferramentas de integraÃ§Ã£o que suportem a carga incremental em vez de cargas totais.
    - Uso de armazenamento em nuvem: Considerar o uso de soluÃ§Ãµes em nuvem escalÃ¡veis.

2. Nosso processo de ETL estÃ¡ gerando dados duplicados e inconsistentes. Como vocÃª resolveria esses problemas para garantir a integridade dos dados?

- Para resolver problemas de duplicaÃ§Ã£o e inconsistÃªncia no ETL:
  - Implementar validaÃ§Ãµes de dados: Durante a extraÃ§Ã£o e transformaÃ§Ã£o, aplicar regras de validaÃ§Ã£o para detectar duplicatas, como verificar chaves primÃ¡rias ou hashes.

3. Nossa ferramenta de visualizaÃ§Ã£o de dashboards estÃ¡ lenta e o nosso time detectou que o problema estÃ¡ na infraestrutura de dados. Como vocÃª abordaria esta situaÃ§Ã£o do ponto vista de arquitetura de dados?
- Abordaria a situaÃ§Ã£o da seguinte forma:
  - Analisar as consultas feitas pelos dashboards e otimizÃ¡-las, utilizando Ã­ndices e evitando joins desnecessÃ¡rios.
  - Considerar o uso de views materializadas para prÃ©-computar resultados e reduzir o tempo de resposta nas consultas mais frequentes.
 
4. Descreva como vocÃª projetaria um pipeline de ETL para integrar os feedbacks coletados em diferentes formatos (planilhas, formulÃ¡rios online, etc.) em um Ãºnico data warehouse.
Eu projetaria um pipeline de ETL da seguinte maneira:
    - ExtraÃ§Ã£o: Utilizar conectores para diferentes fontes de dados (APIs, arquivos CSV, etc.).
    - TransformaÃ§Ã£o: Normalizar os dados coletados, garantindo que todos os formatos sejam convertidos para um padrÃ£o comum (por exemplo, mesmo esquema de colunas).
    - ValidaÃ§Ã£o: Aplicar regras de validaÃ§Ã£o para garantir que os dados sejam consistentes e completos antes de carregÃ¡-los.
    - Carga: Inserir os dados transformados em um data warehouse.
    - OrquestraÃ§Ã£o: Automatizar e agendar as tasks e um espaÃ§o de tempo que faÃ§a sentido para a aplicaÃ§Ã£o.

5. Nossa Ã¡rea operacional necessita de informaÃ§Ãµes em tempo real, porÃ©m os gestores da comunidade acompanham somente informaÃ§Ãµes de KPIs mensais, alegam que isso Ã© desnecessÃ¡rio e acarretaria custos. Qual Ã© o seu posicionamento sobre isso?
    - Acredito que, embora os KPIs mensais sejam valiosos, informaÃ§Ãµes em tempo real podem fornecer insights crÃ­ticos que permitem decisÃµes mais rÃ¡pidas e informadas. Contudo Ã© necessario avaliar se realmente a escalada de custo faz sentido na visÃ£o geral das analises.

6. Como vocÃª implementaria prÃ¡ticas de governanÃ§a de dados para assegurar que os feedbacks sejam armazenados e utilizados de maneira segura e conforme as polÃ­ticas da comunidade?
    - PolÃ­ticas de acesso: Definir quem tem acesso aos dados e em que nÃ­vel, garantindo que apenas pessoas autorizadas possam visualizar ou modificar informaÃ§Ãµes sensÃ­veis (a maior partes dos banco de dados mais utilizados jÃ¡ possuem tal propriedade).

7. Nosso time que estÃ¡ focado em GovernanÃ§a de Dados alega que documentar os processos Ã© mais importante do que refatorar os mais de 500 scripts que estÃ£o funcionando com lentidÃ£o. Como vocÃª atuaria neste impasse, se tivesse que priorizar o trabalho?
- Neste impasse, eu buscaria um equilÃ­brio entre documentaÃ§Ã£o e melhoria de desempenho:
    - DocumentaÃ§Ã£o mÃ­nima viÃ¡vel: Criar documentaÃ§Ã£o bÃ¡sica para os scripts enquanto trabalha na refatoraÃ§Ã£o.

--- 

## ContribuiÃ§Ãµes e Contatos
- **LinkedIn**: [Antonio Junior](https://www.linkedin.com/in/antoniojuniortec/)
- **GitHub**: [AntonioC4r10s](https://github.com/AntonioC4r10s)

Sinta-se Ã  vontade para abrir issues ou pull requests no repositÃ³rio. Estou ansioso para colaborar com vocÃª!
